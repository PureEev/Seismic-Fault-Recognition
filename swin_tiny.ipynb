{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ee106d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install monai torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d3d0ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from monai.networks.nets import SwinUNETR\n",
    "import torch.nn.functional as F\n",
    "from types import MethodType\n",
    "import inspect\n",
    "from typing import Dict, Any, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b103582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_device(device: Optional[torch.device] = None) -> torch.device:\n",
    "    if device is None:\n",
    "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if isinstance(device, str):\n",
    "        return torch.device(device)\n",
    "    return device\n",
    "\n",
    "def build_swinunetr(params: Dict[str, Any],\n",
    "                           in_ch: int = 1,\n",
    "                           out_ch: int = 1,\n",
    "                           device: Optional[torch.device] = None,\n",
    "                           wrap_dataparallel: bool = True) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Сборка SwinUNETR из словаря params:\n",
    "    - фильтрует params по сигнатуре конструктора (чтобы игнорировать лишние ключи);\n",
    "    - создаёт модель, при наличии >1 GPU оборачивает в DataParallel (если wrap_dataparallel=True);\n",
    "    - возвращает модель на указанном device.\n",
    "    \"\"\"\n",
    "    device = _get_device(device)\n",
    "    sig = inspect.signature(SwinUNETR.__init__)\n",
    "    allowed = {p for p in sig.parameters if p not in (\"self\", \"in_channels\", \"out_channels\")}\n",
    "    call_kwargs = {k: v for k, v in params.items() if k in allowed}\n",
    "    model = SwinUNETR(in_ch, out_ch, **call_kwargs)\n",
    "    if wrap_dataparallel and torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    return model.to(device)\n",
    "\n",
    "def modify_swinunetr_model(model):\n",
    "    \"\"\"\n",
    "    1) заменяет swinViT.patch_embed.proj -> Conv3d(..., kernel=5, stride=2, pad=2, bias=False)\n",
    "    2) уменьшает число swin стадий на 1 (удаляет layers4)\n",
    "    3) делает encoder4 новым bottleneck (вместо encoder10) и перестраивает forward, чтобы использовать decoder3..1\n",
    "    4) добавляет 1x1 адаптеры enc2to3 и enc3to4 при необходимости\n",
    "    \"\"\"\n",
    "\n",
    "    # --------- 1) patch_embed.proj replace ----------\n",
    "    if not hasattr(model, 'swinViT') or not hasattr(model.swinViT, 'patch_embed'):\n",
    "        raise AttributeError(\"model не имеет model.swinViT.patch_embed — проверьте имя атрибута.\")\n",
    "    pe = model.swinViT.patch_embed\n",
    "\n",
    "    in_chans = getattr(getattr(pe, 'proj', None), 'in_channels', None)\n",
    "    out_chans = getattr(getattr(pe, 'proj', None), 'out_channels', None)\n",
    "    embed_dim = getattr(pe, 'embed_dim', None) or out_chans\n",
    "\n",
    "    new_proj = nn.Conv3d(\n",
    "        in_channels=int(in_chans),\n",
    "        out_channels=int(embed_dim),\n",
    "        kernel_size=5,\n",
    "        stride=2,\n",
    "        padding=2,\n",
    "        bias=False\n",
    "    )\n",
    "    pe.proj = new_proj\n",
    "    print(f\"[OK] Заменил patch_embed.proj -> Conv3d({in_chans},{embed_dim},kernel=5,stride=2,pad=2,bias=False)\")\n",
    "\n",
    "    # --------- 2) убрать одну swin-стадию (layers4) ----------\n",
    "    swin = model.swinViT\n",
    "\n",
    "    # Найдём layers в порядке\n",
    "    layers = []\n",
    "    for name in dir(swin):\n",
    "        if name.startswith('layers'):\n",
    "            attr = getattr(swin, name)\n",
    "            if isinstance(attr, (nn.Module, nn.ModuleList)):\n",
    "                layers.append((name, attr))\n",
    "\n",
    "    layers = sorted(layers, key=lambda x: x[0])\n",
    "    \n",
    "    \n",
    "    last_name, last_layer = layers[-1]\n",
    "    prev_name, prev_layer = layers[-2]\n",
    "    # удаляем last stage: чтобы forward не падал, оставим placeholder Module \n",
    "    # наш forward не будет использовать swinViT.layers4)\n",
    "    setattr(swin, last_name, nn.Identity())\n",
    "    # уменьшить num_layers если есть\n",
    "    if hasattr(swin, 'num_layers'):\n",
    "        old = int(getattr(swin, 'num_layers'))\n",
    "        swin.num_layers = max(0, old - 1)\n",
    "        print(f\"[INFO] swinViT.num_layers: {old} -> {swin.num_layers}\")\n",
    "        \n",
    "    print(f\"[OK] Обрезал {last_name} (подставил Identity).\")\n",
    "\n",
    "    # --------- 3) удалить/не использовать encoder10/decoder5 (глубочайший) ----------\n",
    "    # если они есть — удалим, чтобы избежать путаницы\n",
    "    removed = []\n",
    "    for name in ('encoder10', 'decoder5'):\n",
    "        if hasattr(model, name):\n",
    "            try:\n",
    "                delattr(model, name)\n",
    "            except Exception:\n",
    "                # если delattr не срабатывает — переопределим как Identity (без изменения forward)\n",
    "                setattr(model, name, nn.Identity())\n",
    "            removed.append(name)\n",
    "    if removed:\n",
    "        print(\"[INFO] Удалены/переопределены глубочайшие модули:\", \", \".join(removed))\n",
    "\n",
    "    # --------- 4) добавим 1x1 адаптеры для согласования каналов  ----------\n",
    "    # нам нужны conv: enc2(48)->enc3(96) и enc3(96)->enc4(192)\n",
    "    model.enc2to3 = nn.Conv3d(48, 96, kernel_size=1, bias=False)\n",
    "    nn.init.kaiming_normal_(model.enc2to3.weight, nonlinearity='relu')\n",
    "    print(\"[OK] Добавлен enc2to3: Conv3d(48->96, k=1)\")\n",
    "\n",
    "    model.enc3to4 = nn.Conv3d(96, 192, kernel_size=1, bias=False)\n",
    "    nn.init.kaiming_normal_(model.enc3to4.weight, nonlinearity='relu')\n",
    "    print(\"[OK] Добавлен enc3to4: Conv3d(96->192, k=1)\")\n",
    "\n",
    "    # --------- 5) Подменяем forward: использовать encoder1..encoder4 как глубокую часть; encoder4 = bottleneck ----------\n",
    "    # Новый forward:\n",
    "    def _new_forward(self, x):\n",
    "        # x: (B, C=1, D, H, W)\n",
    "        # encoder1 (full res)\n",
    "        enc1 = self.encoder1(x)                     # -> 48 ch\n",
    "        # encoder2 (half)\n",
    "        e2_in = F.max_pool3d(enc1, kernel_size=2, stride=2)\n",
    "        enc2 = self.encoder2(e2_in)                 # -> 48 ch\n",
    "        # encoder3 (quarter) - требуется mapping 48->96\n",
    "        e3_in = F.max_pool3d(enc2, kernel_size=2, stride=2)\n",
    "        e3_in_mapped = self.enc2to3(e3_in)         # -> 96 ch\n",
    "        enc3 = self.encoder3(e3_in_mapped)         # -> 96 ch\n",
    "        # encoder4 (eighth) - mapping 96->192\n",
    "        e4_in = F.max_pool3d(enc3, kernel_size=2, stride=2)\n",
    "        e4_in_mapped = self.enc3to4(e4_in)         # -> 192 ch\n",
    "        enc4 = self.encoder4(e4_in_mapped)         # -> 192 ch (новый bottleneck)\n",
    "\n",
    "        d3 = self.decoder3(enc4, enc3)             # in=192 -> out=96, skip enc3 (96)\n",
    "        d2 = self.decoder2(d3, enc2)               # in=96  -> out=48, skip enc2 (48)\n",
    "        d1 = self.decoder1(d2, enc1)               # in=48  -> out=48, skip enc1 (48)\n",
    "\n",
    "        out = self.out(d1)\n",
    "        return out\n",
    "\n",
    "    # Bind new forward to model instance\n",
    "    model.forward = MethodType(_new_forward, model)\n",
    "    print(\"[OK] Подменён model.forward: использую encoder1..encoder4 в качестве 3 уровней + bottleneck=encoder4; \"\n",
    "          \"декодеры используются decoder3->decoder2->decoder1.\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3655673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, checkpoint_path, device):\n",
    "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "  checkpoint_dict = {}\n",
    "  for key, value in checkpoint[\"model_state_dict\"].items():\n",
    "      if key.startswith('module.'):\n",
    "          key = key.replace('module.', '')\n",
    "      checkpoint_dict[key] = value\n",
    "  \n",
    "  model.load_state_dict(checkpoint_dict)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f13665c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Заменил patch_embed.proj -> Conv3d(1,48,kernel=5,stride=2,pad=2,bias=False)\n",
      "[INFO] swinViT.num_layers: 4 -> 3\n",
      "[OK] Обрезал layers4 (подставил Identity).\n",
      "[INFO] Удалены/переопределены глубочайшие модули: encoder10, decoder5\n",
      "[OK] Добавлен enc2to3: Conv3d(48->96, k=1)\n",
      "[OK] Добавлен enc3to4: Conv3d(96->192, k=1)\n",
      "[OK] Подменён model.forward: использую encoder1..encoder4 в качестве 3 уровней + bottleneck=encoder4; декодеры используются decoder3->decoder2->decoder1.\n",
      "torch.Size([1, 1, 128, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/burlaka-na/Downloads/swinunetr_tiny_after_faultseg3d.pth\"\n",
    "\n",
    "MODEL_PARAMS = dict(\n",
    "    patch_size=(2, 2, 2),\n",
    "    depths=(2, 2, 2, 1),\n",
    "    num_heads=(3, 6, 12, 24),\n",
    "    window_size=(7, 7, 7),\n",
    "    qkv_bias=True,\n",
    "    mlp_ratio=4.0,\n",
    "    feature_size=48,\n",
    "    drop_rate=0.0,\n",
    "    attn_drop_rate=0.0,\n",
    "    dropout_path_rate=0.1,\n",
    "    patch_norm=True,\n",
    "    spatial_dims=3,\n",
    ")\n",
    "MAIN_DEVICE = \"cpu\"\n",
    "\n",
    "model = build_swinunetr(MODEL_PARAMS, in_ch=1, out_ch=1)\n",
    "\n",
    "swin_tiny_model = modify_swinunetr_model(model)\n",
    "\n",
    "swin_tiny_model = swin_tiny_model.to(MAIN_DEVICE)\n",
    "\n",
    "swin_tiny_model = load_checkpoint(swin_tiny_model, model_path, MAIN_DEVICE)\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "x = torch.randn(1, 1, 128, 128, 128, device=device)\n",
    "\n",
    "out = swin_tiny_model(x)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7143bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
