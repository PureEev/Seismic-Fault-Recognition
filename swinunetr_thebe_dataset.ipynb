{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2025-10-16T15:06:54.779001Z",
          "iopub.status.busy": "2025-10-16T15:06:54.778434Z",
          "iopub.status.idle": "2025-10-16T15:08:14.923329Z",
          "shell.execute_reply": "2025-10-16T15:08:14.922511Z",
          "shell.execute_reply.started": "2025-10-16T15:06:54.778943Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFr4j2-jwVb2",
        "outputId": "4a1e5d4e-c17c-44e1-960a-97e88fa3fa85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: monai in /usr/local/lib/python3.12/dist-packages (1.5.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.24 in /usr/local/lib/python3.12/dist-packages (from monai) (2.0.2)\n",
            "Requirement already satisfied: torch>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from monai) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.1->monai) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.1->monai) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install monai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-16T15:08:14.925369Z",
          "iopub.status.busy": "2025-10-16T15:08:14.925122Z",
          "iopub.status.idle": "2025-10-16T15:08:49.716584Z",
          "shell.execute_reply": "2025-10-16T15:08:49.715914Z",
          "shell.execute_reply.started": "2025-10-16T15:08:14.925346Z"
        },
        "trusted": true,
        "id": "e_jlCjS0wVb4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import requests\n",
        "from typing import Dict, Optional\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    from tqdm import tqdm\n",
        "    _has_tqdm = True\n",
        "except ImportError:\n",
        "    _has_tqdm = False\n",
        "\n",
        "\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
        "# или попробовать:\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import average_precision_score, f1_score, jaccard_score\n",
        "import warnings\n",
        "import os\n",
        "import random\n",
        "import inspect\n",
        "from typing import List, Tuple, Optional, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "import monai\n",
        "from monai.networks.nets import SwinUNETR\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.metrics import DiceMetric\n",
        "from scipy import ndimage as ndi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T17:45:09.953825Z",
          "iopub.status.busy": "2025-10-21T17:45:09.953486Z",
          "iopub.status.idle": "2025-10-21T17:45:10.682633Z",
          "shell.execute_reply": "2025-10-21T17:45:10.681477Z",
          "shell.execute_reply.started": "2025-10-21T17:45:09.953792Z"
        },
        "trusted": true,
        "id": "DcDK7_pWwVb5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "import gdown\n",
        "import numpy as np\n",
        "\n",
        "def download_and_unpack_gdrive_gdown(url_or_id, dest_folder='.', out_filename=None,\n",
        "                                     save_npy=False, quiet=False):\n",
        "    \"\"\"\n",
        "    Скачивает файл с Google Drive через gdown и распаковывает\n",
        "    \"\"\"\n",
        "    os.makedirs(dest_folder, exist_ok=True)\n",
        "\n",
        "    # если передали только id — соберём URL\n",
        "    if re.fullmatch(r\"[A-Za-z0-9_-]{10,}\", url_or_id):\n",
        "        url = f\"https://drive.google.com/uc?id={url_or_id}\"\n",
        "    else:\n",
        "        url = url_or_id\n",
        "\n",
        "    # имя выходного файла\n",
        "    if out_filename is None:\n",
        "        # попытка взять имя из URL, иначе дать дефолт\n",
        "        if \"id=\" in url:\n",
        "            out_filename = f\"downloaded_{url.split('id=')[-1]}.bin\"\n",
        "        else:\n",
        "            out_filename = os.path.basename(url) or \"downloaded_file.bin\"\n",
        "\n",
        "    out_path = os.path.join(dest_folder, out_filename)\n",
        "\n",
        "    # Скачиваем\n",
        "    if not os.path.exists(out_path):\n",
        "        gdown.download(url, out_path, quiet=quiet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T17:45:10.685708Z",
          "iopub.status.busy": "2025-10-21T17:45:10.685013Z",
          "iopub.status.idle": "2025-10-21T17:46:08.893608Z",
          "shell.execute_reply": "2025-10-21T17:46:08.892588Z",
          "shell.execute_reply.started": "2025-10-21T17:45:10.685668Z"
        },
        "trusted": true,
        "id": "bup7XCp_wVb5"
      },
      "outputs": [],
      "source": [
        "\n",
        "file_id = \"1NOUwAFtU_1KIlf8mQtB5xwSY-3NtYnDw\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "download_and_unpack_gdrive_gdown(url_or_id=url,\n",
        "                                           dest_folder=\"/content\",\n",
        "                                           out_filename=\"seis_train.npz\",\n",
        "                                           quiet=False)\n",
        "\n",
        "out_path = os.path.join(\"/kaggle/working\", \"seis_train.npz\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T17:46:08.895003Z",
          "iopub.status.busy": "2025-10-21T17:46:08.894668Z",
          "iopub.status.idle": "2025-10-21T17:46:50.177820Z",
          "shell.execute_reply": "2025-10-21T17:46:50.176846Z",
          "shell.execute_reply.started": "2025-10-21T17:46:08.894973Z"
        },
        "trusted": true,
        "id": "xoG7bMIlwVb6"
      },
      "outputs": [],
      "source": [
        "\n",
        "file_id = \"1QHE1gn7B8Zt99Cw1KDwyfNv6wqhM5h49\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "fault_val = download_and_unpack_gdrive_gdown(url_or_id=url,\n",
        "                                           dest_folder=\"/content\",\n",
        "                                           out_filename=\"fault_train.npz\",\n",
        "                                           quiet=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T17:46:50.178895Z",
          "iopub.status.busy": "2025-10-21T17:46:50.178585Z",
          "iopub.status.idle": "2025-10-21T17:47:06.904438Z",
          "shell.execute_reply": "2025-10-21T17:47:06.903538Z",
          "shell.execute_reply.started": "2025-10-21T17:46:50.178867Z"
        },
        "trusted": true,
        "id": "qHFa9d5wwVb6"
      },
      "outputs": [],
      "source": [
        "\n",
        "file_id = \"1vVOkYAZkq08CtWosLd27Py4quvAaTjOa\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "fault_val = download_and_unpack_gdrive_gdown(url_or_id=url,\n",
        "                                           dest_folder=\"/content\",\n",
        "                                           out_filename=\"seis_val.npz\",\n",
        "                                           quiet=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T17:47:06.907373Z",
          "iopub.status.busy": "2025-10-21T17:47:06.907043Z",
          "iopub.status.idle": "2025-10-21T17:47:29.667021Z",
          "shell.execute_reply": "2025-10-21T17:47:29.665890Z",
          "shell.execute_reply.started": "2025-10-21T17:47:06.907342Z"
        },
        "trusted": true,
        "id": "JF1WLOo4wVb6"
      },
      "outputs": [],
      "source": [
        "\n",
        "file_id = \"1YIeB6J69RUozpKwWa84m8ZXgpaQMWmP7\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "fault_val = download_and_unpack_gdrive_gdown(url_or_id=url,\n",
        "                                           dest_folder=\"/content\",\n",
        "                                           out_filename=\"fault_val.npz\",\n",
        "                                           quiet=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T17:47:29.671260Z",
          "iopub.status.busy": "2025-10-21T17:47:29.670961Z",
          "iopub.status.idle": "2025-10-21T17:47:29.704785Z",
          "shell.execute_reply": "2025-10-21T17:47:29.703694Z",
          "shell.execute_reply.started": "2025-10-21T17:47:29.671231Z"
        },
        "trusted": true,
        "id": "KxW0LNirwVb6"
      },
      "outputs": [],
      "source": [
        "from typing import Union, Sequence, Tuple, Optional, Dict, Iterable\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "\n",
        "class SeisFaultDataset:\n",
        "    \"\"\"\n",
        "    Датасет для пар (сейсмика, fault) в виде 3D numpy-массивов.\n",
        "\n",
        "    Поддерживаемые источники:\n",
        "    - путь к .npz файлу (будет открыт через np.load(..., mmap_mode='r'))\n",
        "    - dict-like mapping имя -> np.ndarray\n",
        "\n",
        "    Парирование:\n",
        "    - если pairs не переданы, автоматически берётся отсортированное пересечение ключей\n",
        "      между seismic и fault источниками. Если пересечение пусто — бросается исключение.\n",
        "\n",
        "    Основные опции:\n",
        "    - target_shape: размер кропа (z,y,x)\n",
        "    - random_crop: True — случайные кропы, False — кропы с нулевого/центрального начала\n",
        "    - seed: для детерминированности\n",
        "    - normalize_seis: применять z-score нормализацию к сейсмике\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        seis_source: Union[str, Dict[str, np.ndarray]],\n",
        "        fault_source: Union[str, Dict[str, np.ndarray]],\n",
        "        pairs: Optional[Sequence[Tuple[str, str]]] = None,\n",
        "        target_shape: Tuple[int, int, int] = (128, 128, 128),\n",
        "        random_crop: bool = True,\n",
        "        seed: Optional[int] = None,\n",
        "        normalize_seis: bool = True,\n",
        "    ):\n",
        "        # параметры\n",
        "        self.target_shape = tuple(int(x) for x in target_shape)\n",
        "        if len(self.target_shape) != 3:\n",
        "            raise ValueError(\"target_shape должен быть кортежем длины 3\")\n",
        "        self.random_crop = bool(random_crop)\n",
        "        self.normalize_seis = bool(normalize_seis)\n",
        "        self._rng = random.Random(seed)\n",
        "\n",
        "        # загрузка источников (path -> npz mmap или оставляем dict)\n",
        "        self._seis_npz = None\n",
        "        self._fault_npz = None\n",
        "\n",
        "        if isinstance(seis_source, str):\n",
        "            if not os.path.exists(seis_source):\n",
        "                raise FileNotFoundError(seis_source)\n",
        "            self._seis_npz = np.load(seis_source, mmap_mode='r')\n",
        "            seis_source = self._seis_npz\n",
        "\n",
        "        if isinstance(fault_source, str):\n",
        "            if not os.path.exists(fault_source):\n",
        "                raise FileNotFoundError(fault_source)\n",
        "            self._fault_npz = np.load(fault_source, mmap_mode='r')\n",
        "            fault_source = self._fault_npz\n",
        "\n",
        "        # ожидаем mapping name->ndarray\n",
        "        self.seis_source = seis_source\n",
        "        self.fault_source = fault_source\n",
        "\n",
        "        # парами: либо явно, либо пересечение ключей\n",
        "        if pairs is not None:\n",
        "            self.pairs = list(pairs)\n",
        "        else:\n",
        "            seis_keys = self._keys_from_source(self.seis_source)\n",
        "            fault_keys = self._keys_from_source(self.fault_source)\n",
        "\n",
        "            # пары (имя, имя)\n",
        "            self.pairs = [(seis, fault) for seis, fault in zip(seis_keys, fault_keys)]\n",
        "\n",
        "        if len(self.pairs) == 0:\n",
        "            raise ValueError(\"No pairs available\")\n",
        "\n",
        "    # ---- Вспомогательные методы ----\n",
        "\n",
        "    def _keys_from_source(self, src):\n",
        "        \"\"\"Возвращает список ключей для mapping-источника или .files для npz.\"\"\"\n",
        "        if hasattr(src, 'files'):\n",
        "            return list(src.files)\n",
        "        if isinstance(src, dict):\n",
        "            return list(src.keys())\n",
        "        # obj, у которого можно взять ключи через итерацию\n",
        "        try:\n",
        "            return list(src.keys())\n",
        "        except Exception:\n",
        "            raise ValueError(f\"Unsupported source type: {type(src)}\")\n",
        "\n",
        "    def _get_array(self, src, key):\n",
        "        \"\"\"Простая загрузка массива; ожидаем numpy.ndarray 3D.\"\"\"\n",
        "        arr = src[key]\n",
        "        if not isinstance(arr, np.ndarray):\n",
        "            # попытка конвертации (обычно не нужна)\n",
        "            arr = np.asarray(arr)\n",
        "        if arr.ndim != 3:\n",
        "            raise ValueError(f\"Ключ {key}: ожидается 3D массив, получено {arr.ndim}D\")\n",
        "        return arr\n",
        "\n",
        "    def _compute_crop_start(self, shape: Tuple[int, int, int]) -> Tuple[int, int, int]:\n",
        "        \"\"\"Вычислить стартовые индексы для кропа (случайно или ноль/центр).\"\"\"\n",
        "        starts = []\n",
        "        for i in range(3):\n",
        "            max_start = shape[i] - self.target_shape[i]\n",
        "            if max_start <= 0:\n",
        "                starts.append(0)\n",
        "            else:\n",
        "                if self.random_crop:\n",
        "                    starts.append(self._rng.randint(0, max_start))\n",
        "                else:\n",
        "                    # по умолчанию — центрированный кроп если есть запас\n",
        "                    starts.append(max_start // 2)\n",
        "        return tuple(starts)\n",
        "\n",
        "    def _zscore_normalize(self, vol: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Z-score нормализация (для сейсмики).\"\"\"\n",
        "        v = vol.astype(np.float32, copy=False)\n",
        "        mu = v.mean()\n",
        "        sigma = v.std()\n",
        "        eps = 1e-8\n",
        "        return (v - mu) / (sigma + eps)\n",
        "\n",
        "    # ---- интерфейс Dataset ----\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        \"\"\"Вернёт (seis_crop, fault_crop, meta).\"\"\"\n",
        "        if idx < 0:\n",
        "            idx = len(self) + idx\n",
        "        seis_key, fault_key = self.pairs[idx]\n",
        "\n",
        "        seis = self._get_array(self.seis_source, seis_key)\n",
        "        fault = self._get_array(self.fault_source, fault_key)\n",
        "\n",
        "        # берем кроп по форме seismic-а\n",
        "        starts = self._compute_crop_start(seis.shape)\n",
        "        slices = tuple(slice(st, st + ts) for st, ts in zip(starts, self.target_shape))\n",
        "\n",
        "        seis_crop = seis[slices]\n",
        "        fault_crop = fault[slices]\n",
        "\n",
        "        if self.normalize_seis:\n",
        "            seis_crop = self._zscore_normalize(seis_crop)\n",
        "\n",
        "        meta = {\n",
        "            \"seis_key\": seis_key,\n",
        "            \"fault_key\": fault_key,\n",
        "            \"crop_start\": starts,\n",
        "            \"original_shapes\": (seis.shape, fault.shape),\n",
        "        }\n",
        "        return seis_crop, fault_crop, meta\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Закрыть mmap .npz если он был открыт.\"\"\"\n",
        "        if self._seis_npz is not None:\n",
        "            try:\n",
        "                self._seis_npz.close()\n",
        "            except Exception:\n",
        "                pass\n",
        "            self._seis_npz = None\n",
        "        if self._fault_npz is not None:\n",
        "            try:\n",
        "                self._fault_npz.close()\n",
        "            except Exception:\n",
        "                pass\n",
        "            self._fault_npz = None\n",
        "\n",
        "    def __del__(self):\n",
        "        self.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T17:47:29.706093Z",
          "iopub.status.busy": "2025-10-21T17:47:29.705812Z",
          "iopub.status.idle": "2025-10-21T17:53:28.282846Z",
          "shell.execute_reply": "2025-10-21T17:53:28.281336Z",
          "shell.execute_reply.started": "2025-10-21T17:47:29.706073Z"
        },
        "trusted": true,
        "id": "_PTAzbxnwVb7"
      },
      "outputs": [],
      "source": [
        "train_ds = SeisFaultDataset(\"/content/seis_train.npz\", \"/content/fault_train.npz\",\n",
        "                      target_shape=(128,128,128), seed=42)\n",
        "\n",
        "val_ds = SeisFaultDataset(\"/content/seis_val.npz\", \"/content/fault_val.npz\",\n",
        "                      target_shape=(128,128,128), seed=42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-16T15:16:00.367333Z",
          "iopub.status.busy": "2025-10-16T15:16:00.367059Z",
          "iopub.status.idle": "2025-10-16T15:16:00.379886Z",
          "shell.execute_reply": "2025-10-16T15:16:00.379074Z",
          "shell.execute_reply.started": "2025-10-16T15:16:00.367308Z"
        },
        "trusted": true,
        "id": "-3WsCT71wVb7"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(path, model, optimizer=None, epoch=None, extra=None):\n",
        "    \"\"\"\n",
        "    Корректно сохраняет state_dict модели (без 'module.'), optimizer и мета.\n",
        "    \"\"\"\n",
        "    model_to_save = model.module if isinstance(model, nn.DataParallel) else model\n",
        "    ckpt = {\"epoch\": epoch, \"model_state_dict\": model_to_save.state_dict()}\n",
        "    if optimizer is not None:\n",
        "        ckpt[\"optimizer_state_dict\"] = optimizer.state_dict()\n",
        "    if extra:\n",
        "        ckpt.update(extra)\n",
        "    torch.save(ckpt, path)\n",
        "\n",
        "\n",
        "def _strip_module_prefix(state_dict):\n",
        "    # удаляем все префиксы \"module.\" (на случай многократных вхождений)\n",
        "    new_state = {}\n",
        "    for k, v in state_dict.items():\n",
        "        if k.startswith(\"module.\"):\n",
        "            new_state[k[len(\"module.\"):]] = v\n",
        "        else:\n",
        "            new_state[k] = v\n",
        "    return new_state\n",
        "\n",
        "\n",
        "def _move_optimizer_state_to_device(opt_state, device):\n",
        "    # переносим все тензоры в optimizer.state на нужное устройство\n",
        "    for state in opt_state.values():\n",
        "        for k, v in list(state.items()):\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                state[k] = v.to(device)\n",
        "\n",
        "\n",
        "def load_checkpoint(\n",
        "    path,\n",
        "    model,\n",
        "    optimizer=None,\n",
        "    device=torch.device(\"cpu\"),\n",
        "    strict=False,\n",
        "    verbose=True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Загрузка чекпойнта с учётом DataParallel\n",
        "    \"\"\"\n",
        "    # безопасно загружаем на cpu, а потом переносим model/opt на device\n",
        "    ckpt = torch.load(path, map_location=\"cpu\")\n",
        "    if verbose:\n",
        "        print(f\"[ckpt] loaded checkpoint type={type(ckpt)}\")\n",
        "\n",
        "    # извлекаем state_dict модели\n",
        "    if isinstance(ckpt, dict):\n",
        "        state = ckpt.get(\"model_state_dict\", ckpt.get(\"state_dict\", None))\n",
        "        if state is None:\n",
        "            # возможно, чекпойнт — это просто state_dict\n",
        "            # проверим: если все значения — тензоры, считаем это state_dict\n",
        "            if all(isinstance(v, torch.Tensor) for v in ckpt.values()):\n",
        "                state = ckpt\n",
        "            else:\n",
        "                raise RuntimeError(\"Checkpoint dict doesn't contain recognizable model state.\")\n",
        "    else:\n",
        "        raise RuntimeError(\"Checkpoint seems to contain a pickled model object. \"\n",
        "                           \"Prefer saving state_dict instead of full model object.\")\n",
        "\n",
        "    # убираем 'module.' если нужно\n",
        "    state = _strip_module_prefix(state)\n",
        "\n",
        "    # проверяем размеры ключей и предупреждаем о несоответствиях до загрузки\n",
        "    model_to_load = model.module if isinstance(model, nn.DataParallel) else model\n",
        "    model_state = model_to_load.state_dict()\n",
        "    mismatched_shapes = []\n",
        "    for k, v in state.items():\n",
        "        if k in model_state and v.shape != model_state[k].shape:\n",
        "            mismatched_shapes.append((k, v.shape, model_state[k].shape))\n",
        "    if mismatched_shapes and verbose:\n",
        "        print(\"[ckpt] WARNING: found tensors with mismatched shapes (ckpt vs model):\")\n",
        "        for k, s_ckpt, s_model in mismatched_shapes[:10]:\n",
        "            print(f\"   {k}: ckpt{tuple(s_ckpt)} != model{tuple(s_model)}\")\n",
        "        if len(mismatched_shapes) > 10:\n",
        "            print(f\"   ... and {len(mismatched_shapes)-10} more\")\n",
        "\n",
        "    # загружаем state_dict (strict может быть False, чтобы не падать)\n",
        "    load_res = model_to_load.load_state_dict(state, strict=strict)\n",
        "    if verbose:\n",
        "        # load_state_dict возвращает NamedTuple(missing_keys, unexpected_keys)\n",
        "        print(\"[ckpt] load_state_dict result:\", load_res)\n",
        "\n",
        "    # переносим модель на device\n",
        "    model.to(device)\n",
        "\n",
        "    # если есть optimizer и он сохранялся — грузим и переносим тензоры оптимизатора\n",
        "    if optimizer is not None and isinstance(ckpt, dict) and \"optimizer_state_dict\" in ckpt:\n",
        "        optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
        "        _move_optimizer_state_to_device(optimizer.state, device)\n",
        "        if verbose:\n",
        "            print(\"[ckpt] optimizer state loaded and moved to\", device)\n",
        "\n",
        "    return ckpt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tikdGA72wVb8"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "from typing import Dict, Any, Optional\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def _get_device(device: Optional[torch.device] = None) -> torch.device:\n",
        "    if device is None:\n",
        "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    if isinstance(device, str):\n",
        "        return torch.device(device)\n",
        "    return device\n",
        "\n",
        "def build_swinunetr(params: Dict[str, Any],\n",
        "                           in_ch: int = 1,\n",
        "                           out_ch: int = 1,\n",
        "                           device: Optional[torch.device] = None,\n",
        "                           wrap_dataparallel: bool = True) -> nn.Module:\n",
        "    \"\"\"\n",
        "    Сборка SwinUNETR из словаря params:\n",
        "    - фильтрует params по сигнатуре конструктора (чтобы игнорировать лишние ключи);\n",
        "    - создаёт модель, при наличии >1 GPU оборачивает в DataParallel (если wrap_dataparallel=True);\n",
        "    - возвращает модель на указанном device.\n",
        "    \"\"\"\n",
        "    device = _get_device(device)\n",
        "    sig = inspect.signature(SwinUNETR.__init__)\n",
        "    allowed = {p for p in sig.parameters if p not in (\"self\", \"in_channels\", \"out_channels\")}\n",
        "    call_kwargs = {k: v for k, v in params.items() if k in allowed}\n",
        "    model = SwinUNETR(in_ch, out_ch, **call_kwargs)\n",
        "    if wrap_dataparallel and torch.cuda.device_count() > 1:\n",
        "        model = nn.DataParallel(model)\n",
        "    return model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-16T15:16:00.381205Z",
          "iopub.status.busy": "2025-10-16T15:16:00.380935Z",
          "iopub.status.idle": "2025-10-16T16:26:43.027994Z",
          "shell.execute_reply": "2025-10-16T16:26:43.027090Z",
          "shell.execute_reply.started": "2025-10-16T15:16:00.381185Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXFSWMGrwVb8",
        "outputId": "8613081e-d03c-4bce-b6af-70ea34271a3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E1:   2%|▏         | 31/1596 [01:39<1:21:39,  3.13s/it, loss=1.64]"
          ]
        }
      ],
      "source": [
        "\n",
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, weight_bce=1.0, weight_dice=1.0):\n",
        "        super().__init__()\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.bce = nn.BCEWithLogitsLoss(\n",
        "            pos_weight=torch.tensor([13.3], device=device)\n",
        "        )\n",
        "        self.dice = monai.losses.DiceLoss(sigmoid=True, squared_pred=False, reduction=\"mean\")\n",
        "        self.w_bce = weight_bce\n",
        "        self.w_dice = weight_dice\n",
        "\n",
        "    def forward(self, logits, target):\n",
        "        if target.dim() == logits.dim() - 1:\n",
        "            target = target.unsqueeze(1)\n",
        "        return self.w_bce * self.bce(logits, target) + self.w_dice * self.dice(logits, target)\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, loss_fn, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Train E{epoch}\")\n",
        "\n",
        "    for i, batch in pbar:\n",
        "        if len(batch) == 3:\n",
        "            imgs, masks, metas = batch\n",
        "        else:\n",
        "            imgs, masks = batch\n",
        "\n",
        "        imgs = imgs.unsqueeze(1).float()\n",
        "        masks = masks.unsqueeze(1).float()\n",
        "\n",
        "        imgs = imgs.to(DEVICE, non_blocking=True)\n",
        "        masks = masks.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        try:\n",
        "            outputs = model(imgs)\n",
        "        except ValueError as e:\n",
        "            msg = str(e)\n",
        "            if \"Expected more than 1 spatial element\" in msg:\n",
        "                raise RuntimeError(\"Runtime forward error: spatial dims too small. Consider increasing ROI or manual model changes.\\n\" + msg)\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "        loss = loss_fn(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += float(loss.item())\n",
        "        pbar.set_postfix(loss=running_loss / (i + 1))\n",
        "\n",
        "    return running_loss / len(dataloader)\n",
        "\n",
        "def validate(model, dataloader):\n",
        "    model.eval()\n",
        "    loss_fn = CombinedLoss()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    FN = 0\n",
        "    total_pixels = 0\n",
        "\n",
        "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=\"Val\")\n",
        "    with torch.no_grad():\n",
        "        for i, batch in pbar:\n",
        "            if len(batch) == 3:\n",
        "                imgs, masks, metas = batch\n",
        "            else:\n",
        "                imgs, masks = batch\n",
        "\n",
        "            imgs = imgs.unsqueeze(1).float().to(DEVICE)\n",
        "            masks = masks.unsqueeze(1).float().to(DEVICE)\n",
        "\n",
        "            logits = sliding_window_inference(\n",
        "                inputs=imgs,\n",
        "                roi_size=ROI_SIZE,\n",
        "                sw_batch_size=SW_BATCH_SIZE,\n",
        "                predictor=model,\n",
        "                overlap=OVERLAP,\n",
        "            )\n",
        "\n",
        "            # loss\n",
        "            val_loss += float(loss_fn(logits, masks).item())\n",
        "\n",
        "            # preds\n",
        "            probs = torch.sigmoid(logits)\n",
        "            preds = (probs >= 0.5).float()\n",
        "\n",
        "            # приводим к 1D\n",
        "            preds_flat = preds.view(-1)\n",
        "            masks_flat = masks.view(-1)\n",
        "\n",
        "            # аккуратно считаем TP/FP/FN на GPU, затем переводим в int\n",
        "            TP += int(((preds_flat == 1.0) & (masks_flat == 1.0)).sum().item())\n",
        "            FP += int(((preds_flat == 1.0) & (masks_flat == 0.0)).sum().item())\n",
        "            FN += int(((preds_flat == 0.0) & (masks_flat == 1.0)).sum().item())\n",
        "\n",
        "            total_pixels += preds_flat.numel()\n",
        "\n",
        "    # усреднённый лосс по батчам\n",
        "    val_loss = val_loss / max(1, len(dataloader))\n",
        "\n",
        "    # F1 (без деления на ноль)\n",
        "    denom = 2 * TP + FP + FN\n",
        "    if denom == 0:\n",
        "        # нет позитивных пикселей ни в GT, ни в предсказаниях -> F1 не определён\n",
        "        val_f1 = float(\"nan\")\n",
        "    else:\n",
        "        val_f1 = 2.0 * TP / denom\n",
        "\n",
        "    return val_loss, val_f1\n",
        "\n",
        "\n",
        "ROI_SIZE = (128, 128, 128)\n",
        "LEARNING_RATE = 1e-4\n",
        "ROOT_DIR = \"./checkpoints\"\n",
        "SW_BATCH_SIZE = 2\n",
        "OVERLAP = 0.25\n",
        "\n",
        "MODEL_PARAMS = dict(\n",
        "    patch_size=(2, 2, 2),\n",
        "    depths=(2, 2, 2, 1),\n",
        "    num_heads=(3, 6, 12, 24),\n",
        "    window_size=(7, 7, 7),\n",
        "    qkv_bias=True,\n",
        "    mlp_ratio=4.0,\n",
        "    feature_size=48,\n",
        "    drop_rate=0.0,\n",
        "    attn_drop_rate=0.0,\n",
        "    dropout_path_rate=0.1,\n",
        "    patch_norm=True,\n",
        "    spatial_dims=3,\n",
        ")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "\n",
        "# детерминированный рандом-генератор для reproducibility\n",
        "generator = torch.Generator()\n",
        "generator.manual_seed(SEED)\n",
        "\n",
        "#train_ds, val_ds, out = torch.utils.data.random_split(ds, [train_len, val_len, out_len], generator=generator)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "model = build_swinunetr(MODEL_PARAMS, in_ch=1, out_ch=1)\n",
        "\n",
        "#--- подгрузка чек поинта\n",
        "#ckpt_path = \"/content/checkpoint.pth\"\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
        "\n",
        "#ckpt = load_checkpoint(ckpt_path, model, optimizer=optimizer, device=DEVICE, strict=False, verbose=True)\n",
        "# после загрузки\n",
        "#model_to_inspect = model.module if isinstance(model, nn.DataParallel) else model\n",
        "#print(\"Model param count:\", sum(p.numel() for p in model_to_inspect.parameters()))\n",
        "# сравнить ключи чекпойнта и модели\n",
        "#ckpt_state = ckpt.get(\"model_state_dict\", ckpt.get(\"state_dict\", ckpt))\n",
        "#ckpt_keys = set(k.replace(\"module.\", \"\") for k in ckpt_state.keys())\n",
        "#model_keys = set(model_to_inspect.state_dict().keys())\n",
        "#print(\"keys in ckpt but not in model:\", sorted(list(ckpt_keys - model_keys))[:20])\n",
        "#print(\"keys in model but not in ckpt:\", sorted(list(model_keys - ckpt_keys))[:20])\n",
        "#--- подгрузка чек поинта\n",
        "\n",
        "loss_fn = CombinedLoss()\n",
        "best_val_dice = -1.0\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_dices = []\n",
        "val_f1s = []\n",
        "val_aps = []\n",
        "val_ious = []\n",
        "\n",
        "best_val_dice = -1.0\n",
        "best_val_f1 = 0\n",
        "\n",
        "os.makedirs(ROOT_DIR, exist_ok=True)\n",
        "loss_plot_path = os.path.join(ROOT_DIR, \"training_loss.png\")\n",
        "\n",
        "for epoch in range(1, 2):\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, loss_fn, epoch)\n",
        "\n",
        "    val_loss, val_f1 = validate(model, val_loader)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch} | Train loss: {train_loss:.4f} | Val loss: {val_loss:.4f} | \"\n",
        "        f\"F1: {val_f1 if not np.isnan(val_f1) else 'nan'}\"\n",
        "    )\n",
        "\n",
        "    model_to_save = model.module if isinstance(model, nn.DataParallel) else model\n",
        "    checkpoint = {\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state_dict\": model_to_save.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"val_f1\": val_f1,\n",
        "        \"val_loss\": val_loss,\n",
        "    }\n",
        "    torch.save(checkpoint, os.path.join(ROOT_DIR, f\"checkpoint_epoch_{epoch}.pth\"))\n",
        "\n",
        "    # обновим лучший по F1\n",
        "    if (not np.isnan(val_f1) and val_f1 > best_val_f1):\n",
        "        best_val_f1 = val_f1\n",
        "        torch.save(checkpoint, os.path.join(ROOT_DIR, \"best_checkpoint.pth\"))\n",
        "        print(\"Best model updated:\", best_val_f1)\n",
        "         # обновляем исторические списки\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    val_f1s.append(val_f1)\n",
        "\n",
        "    # рисуем и сохраняем график loss (train + val)\n",
        "    try:\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        epochs_range = list(range(1, len(train_losses) + 1))\n",
        "        plt.plot(epochs_range, train_losses, label=\"Train Loss\")\n",
        "        plt.plot(epochs_range, val_losses, label=\"Val Loss\")\n",
        "        plt.title(\"Training and Validation Loss\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        # сохраняем в верхнюю директорию (ROOT_DIR)\n",
        "        plt.savefig(loss_plot_path)\n",
        "        plt.show()  # в ноутбуке Kaggle покажет график\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"Failed to plot/save loss figure: {e}\")\n",
        "\n",
        "print(\"Training finished. Best val f1:\", best_val_f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GrGfhzHwVb9"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8S6JKG1hyA13"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31089,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}